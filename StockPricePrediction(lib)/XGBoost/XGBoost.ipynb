{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:14.033849Z","iopub.status.busy":"2024-05-31T08:05:14.032965Z","iopub.status.idle":"2024-05-31T08:05:15.300916Z","shell.execute_reply":"2024-05-31T08:05:15.299911Z","shell.execute_reply.started":"2024-05-31T08:05:14.033777Z"},"trusted":true},"outputs":[],"source":["from sklearn.datasets import load_iris, load_diabetes\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold, TimeSeriesSplit, ParameterGrid\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score\n","from xgboost import XGBClassifier, XGBRegressor\n","from xgboost import plot_tree\n","from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error, r2_score\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.303219Z","iopub.status.busy":"2024-05-31T08:05:15.302494Z","iopub.status.idle":"2024-05-31T08:05:15.341547Z","shell.execute_reply":"2024-05-31T08:05:15.340353Z","shell.execute_reply.started":"2024-05-31T08:05:15.303170Z"},"trusted":true},"outputs":[],"source":["filepath = 'E:\\C\\ICT\\Machine Learning and Data Mining\\ML-DM-Stock-Price-Prediction\\Dataset\\VNM.csv'\n","\n","df = pd.read_csv(filepath)\n","df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.343165Z","iopub.status.busy":"2024-05-31T08:05:15.342852Z","iopub.status.idle":"2024-05-31T08:05:15.352673Z","shell.execute_reply":"2024-05-31T08:05:15.351459Z","shell.execute_reply.started":"2024-05-31T08:05:15.343131Z"},"trusted":true},"outputs":[],"source":["scaler = StandardScaler()\n","# scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(np.array(df['close']).reshape(-1,1))\n","# scaled_data = scaled_data.reshape(scaled_data.shape[0])\n","print(scaled_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.356560Z","iopub.status.busy":"2024-05-31T08:05:15.356213Z","iopub.status.idle":"2024-05-31T08:05:15.366121Z","shell.execute_reply":"2024-05-31T08:05:15.365063Z","shell.execute_reply.started":"2024-05-31T08:05:15.356533Z"},"trusted":true},"outputs":[],"source":["# Generate dataset\n","def data_generator(datasets, timestep):\n","    dataset_inp = []\n","    dataset_oup = []\n","    for day in range(timestep, len(datasets)):\n","        subsample = []\n","        for prev_day in range(day-timestep, day):\n","            subsample.append(datasets[prev_day])\n","        dataset_inp.append(subsample)\n","        dataset_oup.append(datasets[day])\n","    return (np.array(dataset_inp), np.array(dataset_oup).reshape(-1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.367714Z","iopub.status.busy":"2024-05-31T08:05:15.367396Z","iopub.status.idle":"2024-05-31T08:05:15.383587Z","shell.execute_reply":"2024-05-31T08:05:15.382377Z","shell.execute_reply.started":"2024-05-31T08:05:15.367690Z"},"trusted":true},"outputs":[],"source":["train_size = 1500\n","\n","data_for_train, data_for_test = scaled_data[: train_size], scaled_data[train_size: ]\n","print(data_for_train.shape)\n","print(data_for_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.385630Z","iopub.status.busy":"2024-05-31T08:05:15.384823Z","iopub.status.idle":"2024-05-31T08:05:15.530019Z","shell.execute_reply":"2024-05-31T08:05:15.528796Z","shell.execute_reply.started":"2024-05-31T08:05:15.385605Z"},"trusted":true},"outputs":[],"source":["timestep = 30 # number of previous day that today depend on\n","newX_train, newY_train = data_generator(data_for_train, timestep)\n","newX_test, newY_test = data_generator(data_for_test, timestep)\n","newX_train = newX_train.reshape(newX_train.shape[0], -1)\n","newX_test = newX_test.reshape(newX_test.shape[0], -1)\n","print(newX_train.shape, newX_test.shape)\n","print(newY_train.shape, newY_test.shape)\n","print(newX_train[-1], newX_test[0])\n","print(newY_train[-1], newY_test[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.531584Z","iopub.status.busy":"2024-05-31T08:05:15.531274Z","iopub.status.idle":"2024-05-31T08:05:15.536452Z","shell.execute_reply":"2024-05-31T08:05:15.535364Z","shell.execute_reply.started":"2024-05-31T08:05:15.531558Z"},"trusted":true},"outputs":[],"source":["# xgboost = XGBRegressor()\n","# max_depth = [2, 10, 20, 50]\n","# n_estimators = [10, 50, 100, 200, 500]\n","# learning_rate = [0.0001, 0.001, 0.01, 0.1]\n","# param_grid = dict(max_depth = max_depth, n_estimators = n_estimators,\n","#                  learning_rate = learning_rate)\n","\n","# tscv = TimeSeriesSplit(n_splits=10)\n","# kfold = KFold(n_splits=10)\n","\n","# grid_search = GridSearchCV(xgboost, param_grid, n_jobs=-1, cv=tscv,\n","#                           scoring='r2', verbose=0,\n","#                            return_train_score=True)\n","# grid_result = grid_search.fit(newX_train, newY_train)\n","# print(f\"Best {grid_result.best_score_} using {grid_result.best_params_}\")"]},{"cell_type":"markdown","metadata":{},"source":["Some important parameters of the model:\n","\n","– **learning_rate α**: The learning rate of the model, similar to the learning rate in the gradient descent algorithm.\n","\n","– **max_depth**: The maximum depth of each tree in the model.\n","\n","– **gamma**: The regularization term used for pruning a tree.\n","\n","– **n_estimators**: The number of trees in the model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T08:05:15.538247Z","iopub.status.busy":"2024-05-31T08:05:15.537786Z"},"trusted":true},"outputs":[],"source":["# Define the model and the parameter grid\n","# xgboost = XGBRegressor()\n","# max_depth = [2, 10, 20, 50]\n","# n_estimators = [10, 50, 100, 200, 500]\n","# learning_rate = [0.0001, 0.001, 0.01, 0.1]\n","# param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate)\n","xgboost = XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1)\n","max_depth = [2, 10, 20, 50]\n","n_estimators = [10, 50, 100, 200, 500]\n","learning_rate = [0.0001, 0.001, 0.01, 0.1]\n","gamma = [0, 0.1, 1.0]\n","\n","param_grid = dict(\n","    max_depth=max_depth, \n","    n_estimators=n_estimators, \n","    learning_rate=learning_rate,\n","    gamma=gamma\n",")\n","# Define the cross-validation strategy\n","tscv = TimeSeriesSplit(n_splits=10)\n","\n","# Define the scoring metric\n","mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n","\n","# Custom function to print each fold and parameters being evaluated\n","def print_grid_search_details(param_grid, X_train, y_train):\n","    n_samples = X_train.shape[0]\n","    n_folds = tscv.get_n_splits()\n","    fold_size = n_samples // n_folds\n","    print(f\"Size train set: {X_train.shape}\")\n","    print(f\"Size of each fold: {fold_size}\\n\")\n","    \n","    split_idx = 1\n","    for train_idx, valid_idx in tscv.split(X_train):\n","        print(f\"Splitting the first {split_idx} chunks at {split_idx}/{n_folds}\")\n","        split_idx += 1\n","\n","    params_list = list(ParameterGrid(param_grid))\n","    best_params = None\n","    best_score = float('inf')\n","    for idx, params in enumerate(params_list, start=1):\n","        print(f\"Evaluating parameters {idx}: {params}\")\n","        \n","        for train_idx, valid_idx in tscv.split(X_train):\n","            X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n","            y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n","            model = xgboost.set_params(**params)\n","            model.fit(X_tr, y_tr)\n","            y_tr_pred = model.predict(X_tr)\n","            y_val_pred = model.predict(X_val)\n","            train_mse = mean_squared_error(y_tr, y_tr_pred)\n","            valid_mse = mean_squared_error(y_val, y_val_pred)\n","            train_r2 = r2_score(y_tr, y_tr_pred)\n","            valid_r2 = r2_score(y_val, y_val_pred)\n","            \n","        print(f\"Parameters: {params}\")\n","        print(f\"Train Accuracy: {train_r2}\")\n","        print(f\"Valid Accuracy: {valid_r2}\")\n","        print(f\"Train MSE: {train_mse}\")\n","        print(f\"Valid MSE: {valid_mse}\")\n","        print(\"-\" * 64)\n","        \n","        if valid_mse < best_score:\n","            best_score = valid_mse\n","            best_params = params\n","    \n","    return best_params\n","\n","# Perform the grid search with custom print function\n","best_params = print_grid_search_details(param_grid, newX_train, newY_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","xgb = XGBRegressor(learning_rate=best_params['learning_rate'], \n","                   max_depth=best_params['max_depth'],\n","                   n_estimators=best_params['n_estimators'], n_jobs=-1,\n","                  objective=\"reg:squarederror\",\n","                  reg_lambda=0.1)\n","xgb.fit(newX_train, newY_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["newy_pred = xgb.predict(newX_test).reshape(-1, 1)\n","newy_pred_rt = scaler.inverse_transform(newy_pred)\n","newY_test_rt = scaler.inverse_transform(newY_test)\n","print(newy_pred.shape)\n","plt.plot(newy_pred_rt, color='red', label='Test Predict')\n","plt.plot(newY_test_rt, color='blue', label='Test Real')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate train set\n","rmse_train = mean_squared_error(newY_train, xgb.predict(newX_train))\n","mae_train = mean_absolute_error(newY_train, xgb.predict(newX_train))\n","r2_train = r2_score(newY_train, xgb.predict(newX_train))\n","print(\"MAE train: \", mae_train)\n","print(\"RMSE train: \", rmse_train)\n","print(\"R2 train: \", r2_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate test set\n","rmse_test = mean_squared_error(newY_test, newy_pred)\n","mae_test = mean_absolute_error(newY_test, newy_pred)\n","r2_test = r2_score(newY_test, newy_pred)\n","print(\"MAE test: \", mae_test)\n","print(\"RMSE test: \", rmse_test)\n","print(\"R2 test: \", r2_test)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5082255,"sourceId":8513288,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
